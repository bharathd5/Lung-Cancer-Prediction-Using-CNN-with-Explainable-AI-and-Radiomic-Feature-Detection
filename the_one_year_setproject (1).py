# -*- coding: utf-8 -*-
"""The ONE YEAR-SetProject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gyKo3tnVowWczLyEua6-EgZ148FlM6w6
"""

from google.colab import drive
drive.mount('/content/drive')

#  Install necessary libraries (fixed installation commands)
!pip install scikit-learn imbalanced-learn tensorflow keras plotly opencv-python-headless pillow

#  Proper imports
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import cv2
import os
import random
import imageio
from collections import Counter

#  Plotly & Visualization
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots

#  Sklearn (Machine Learning & Metrics)
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score, RepeatedStratifiedKFold
from sklearn.neighbors import LocalOutlierFactor
from sklearn.metrics import (
    accuracy_score, recall_score, precision_score, classification_report, confusion_matrix
)
from sklearn.metrics import ConfusionMatrixDisplay  #  Replaces deprecated plot_confusion_matrix

#Imbalanced data handling
from imblearn.over_sampling import SMOTE

#TensorFlow & Keras (Deep Learning)
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, GlobalAveragePooling2D, BatchNormalization
from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array

#EfficientNet models
from tensorflow.keras.applications import (
    EfficientNetB0, EfficientNetB1, EfficientNetB2, EfficientNetB3, EfficientNetB4,
    EfficientNetB5, EfficientNetB6, EfficientNetB7
)

#ResNet model
from tensorflow.keras.applications.resnet import ResNet50

directory = "/content/drive/MyDrive/TheOne/The IQ-OTHNCCD lung cancer dataset"

categories = ['Bengin cases', 'Malignant cases', 'Normal cases']

size_data = {}
for i in categories:
    path = os.path.join(directory, i)
    class_num = categories.index(i)
    temp_dict = {}
    for file in os.listdir(path):
        filepath = os.path.join(path, file)
        height, width, channels = imageio.imread(filepath).shape
        if str(height) + ' x ' + str(width) in temp_dict:
            temp_dict[str(height) + ' x ' + str(width)] += 1
        else:
            temp_dict[str(height) + ' x ' + str(width)] = 1

    size_data[i] = temp_dict

size_data

for i in categories:
    path = os.path.join(directory, i)
    class_num = categories.index(i)
    for file in os.listdir(path):
        filepath = os.path.join(path, file)
        print(i)
        img = cv2.imread(filepath, 0)
        plt.imshow(img)
        plt.show()
        break

img_size = 256
for i in categories:
    cnt, samples = 0, 3
    fig, ax = plt.subplots(samples, 3, figsize=(15, 15))
    fig.suptitle(i)

    path = os.path.join(directory, i)
    class_num = categories.index(i)
    for curr_cnt, file in enumerate(os.listdir(path)):
        filepath = os.path.join(path, file)
        img = cv2.imread(filepath, 0)

        img0 = cv2.resize(img, (img_size, img_size))

        img1 = cv2.GaussianBlur(img0, (5, 5), 0)

        ax[cnt, 0].imshow(img)
        ax[cnt, 1].imshow(img0)
        ax[cnt, 2].imshow(img1)
        cnt += 1
        if cnt == samples:
            break

plt.show()

data = []
img_size = 256

for i in categories:
    path = os.path.join(directory, i)
    class_num = categories.index(i)
    for file in os.listdir(path):
        filepath = os.path.join(path, file)
        img = cv2.imread(filepath, 0)
        # preprocess here
        img = cv2.resize(img, (img_size, img_size))
        data.append([img, class_num])

random.shuffle(data)

X, y = [], []
for feature, label in data:
    X.append(feature)
    y.append(label)

print('X length:', len(X))
print('y counts:', Counter(y))

# normalize
X = np.array(X).reshape(-1, img_size, img_size, 1)
X = X / 255.0
y = np.array(y)


X_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=10, stratify=y)

print(len(X_train), X_train.shape)
print(len(X_valid), X_valid.shape)

print(Counter(y_train), Counter(y_valid))

print(len(X_train), X_train.shape)

X_train = X_train.reshape(X_train.shape[0], img_size*img_size*1)

print(len(X_train), X_train.shape)

print('Before SMOTE:', Counter(y_train))
smote = SMOTE()
X_train_sampled, y_train_sampled = smote.fit_resample(X_train, y_train)
print('After SMOTE:', Counter(y_train_sampled))



X_train = X_train.reshape(X_train.shape[0], img_size, img_size, 1)
X_train_sampled = X_train_sampled.reshape(X_train_sampled.shape[0], img_size, img_size, 1)

print(len(X_train), X_train.shape)
print(len(X_train_sampled), X_train_sampled.shape)

model1 = Sequential()

model1.add(Conv2D(64, (3, 3), input_shape=X_train.shape[1:]))
model1.add(Activation('relu'))
model1.add(MaxPooling2D(pool_size=(2, 2)))

model1.add(Conv2D(64, (3, 3), activation='relu'))
model1.add(MaxPooling2D(pool_size=(2, 2)))

model1.add(Flatten())
model1.add(Dense(16))
model1.add(Dense(3, activation='softmax'))

model1.summary()

model1.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

history = model1.fit(X_train_sampled, y_train_sampled, batch_size=8, epochs=10, validation_data=(X_valid, y_valid))

y_pred = model1.predict(X_valid, verbose=1)
y_pred_bool = np.argmax(y_pred, axis=1)

print(classification_report(y_valid, y_pred_bool))

print(confusion_matrix(y_true=y_valid, y_pred=y_pred_bool))

plt.plot(history.history['accuracy'], label='Train')
plt.plot(history.history['val_accuracy'], label='Validation')
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend()
plt.show()

plt.plot(history.history['loss'], label='Train')
plt.plot(history.history['val_loss'], label='Validation')
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend()
plt.show()

model2 = Sequential()

model2.add(Conv2D(64, (3, 3), input_shape=X_train.shape[1:]))
model2.add(Activation('relu'))
model2.add(MaxPooling2D(pool_size=(2, 2)))

model2.add(Conv2D(64, (3, 3), activation='relu'))
model2.add(MaxPooling2D(pool_size=(2, 2)))

model2.add(Flatten())
model2.add(Dense(16))
model2.add(Dense(3, activation='softmax'))

model2.summary()


model2.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

new_weights = {
    0: X_train.shape[0]/(3*Counter(y_train)[0]),
    1: X_train.shape[0]/(3*Counter(y_train)[1]),
    2: X_train.shape[0]/(3*Counter(y_train)[2]),
}

# new_weights[0] = 0.5
# new_weights[1] = 20

new_weights


history = model2.fit(X_train, y_train, batch_size=8, epochs=10, validation_data=(X_valid, y_valid), class_weight=new_weights)

y_pred = model2.predict(X_valid, verbose=1)
y_pred_bool = np.argmax(y_pred, axis=1)

print(classification_report(y_valid, y_pred_bool))

print(confusion_matrix(y_true=y_valid, y_pred=y_pred_bool))

plt.plot(history.history['accuracy'], label='Train')
plt.plot(history.history['val_accuracy'], label='Validation')
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend()
plt.show()

plt.plot(history.history['loss'], label='Train')
plt.plot(history.history['val_loss'], label='Validation')
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend()
plt.show()

train_datagen = ImageDataGenerator(horizontal_flip=True, vertical_flip=True)
val_datagen = ImageDataGenerator()

train_generator = train_datagen.flow(X_train, y_train, batch_size=8)
val_generator = val_datagen.flow(X_valid, y_valid, batch_size=8)

model3 = Sequential()

model3.add(Conv2D(64, (3, 3), input_shape=X_train.shape[1:]))
model3.add(Activation('relu'))
model3.add(MaxPooling2D(pool_size=(2, 2)))

model3.add(Conv2D(64, (3, 3), activation='relu'))
model3.add(MaxPooling2D(pool_size=(2, 2)))

model3.add(Flatten())
model3.add(Dense(16))
model3.add(Dense(3, activation='softmax'))

model3.summary()

model3.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Replace model3.fit_generator with model3.fit
history = model3.fit(
    train_generator,
    epochs=5,
    validation_data=val_generator,
    class_weight=new_weights
)

y_pred = model3.predict(X_valid, verbose=1)
y_pred_bool = np.argmax(y_pred, axis=1)

print(classification_report(y_valid, y_pred_bool))

print(confusion_matrix(y_true=y_valid, y_pred=y_pred_bool))

"""#Lung CT Image Prediction"""

#Predicts a Single CT Scan Image

# Mapping label indices to category names
label_map = {0: 'Benign', 1: 'Malignant', 2: 'Normal'}

def predict_ct_scan(image_path, model, img_size=256):
    """
    Loads a CT scan image, preprocesses it, and predicts the class using the given model.
    """
    # Load and preprocess the image
    img = cv2.imread(image_path, 0)  # Grayscale
    img_resized = cv2.resize(img, (img_size, img_size))
    img_normalized = img_resized / 255.0
    img_input = img_normalized.reshape(1, img_size, img_size, 1)  # Add batch and channel dims

    # Predict
    pred = model.predict(img_input)
    predicted_class = np.argmax(pred)

    # Show the image and prediction
    plt.imshow(img_resized, cmap='gray')
    plt.title(f"Predicted: {label_map[predicted_class]}")
    plt.axis('off')
    plt.show()

    return label_map[predicted_class]

image_path = "/content/drive/MyDrive/TheOne/The IQ-OTHNCCD lung cancer dataset/Malignant cases/Malignant case (25).jpg"
prediction = predict_ct_scan(image_path, model3)  # or model1/model2
print("Prediction:", prediction)

"""#Implementing Grad-CAM visualization for identifying lesions

"""

#[Grad-CAM Compatible with Sequential Model] Modified Prediction Function
!pip install tf-keras-vis

import cv2
import numpy as np
import matplotlib.pyplot as plt
from tf_keras_vis.gradcam import Gradcam
from tf_keras_vis.utils.scores import CategoricalScore
import tensorflow as tf

label_map = {0: 'Benign', 1: 'Malignant', 2: 'Normal'}

def predict_ct_scan_with_gradcam(image_path, model, img_size=256):
    """
    Predicts the class of a CT scan using the provided CNN model and visualizes the Grad-CAM heatmap.
    """
    # Load and preprocess the image
    img = cv2.imread(image_path, 0)
    img_resized = cv2.resize(img, (img_size, img_size))
    img_normalized = img_resized.astype("float32") / 255.0
    img_input = img_normalized.reshape(1, img_size, img_size, 1)

    # Predict class
    pred = model.predict(img_input)
    predicted_class = np.argmax(pred)

    # Prepare input for Grad-CAM with 1 channel (do not convert to 3 channels)
    def model_modifier(m):
        m.layers[-1].activation = tf.keras.activations.linear

    score = CategoricalScore(predicted_class)
    gradcam = Gradcam(model, model_modifier=model_modifier, clone=True)

    # Use correct penultimate Conv2D layer (usually -4)
    cam = gradcam(score, img_input, penultimate_layer=-4)
    heatmap = cam[0]
    heatmap_rescaled = cv2.resize(heatmap, (img_size, img_size))
    heatmap_color = cv2.applyColorMap(np.uint8(255 * heatmap_rescaled), cv2.COLORMAP_JET)
    superimposed_img = heatmap_color * 0.4 + cv2.cvtColor(img_resized, cv2.COLOR_GRAY2BGR)

    # Visualization
    plt.figure(figsize=(12, 5))
    plt.subplot(1, 3, 1)
    plt.title('Original Image')
    plt.imshow(img_resized, cmap='gray')
    plt.axis('off')

    plt.subplot(1, 3, 2)
    plt.title('Grad-CAM Heatmap')
    plt.imshow(heatmap_rescaled, cmap='jet')
    plt.axis('off')

    plt.subplot(1, 3, 3)
    plt.title(f'Overlay - Predicted: {label_map[predicted_class]}')
    plt.imshow(superimposed_img.astype(np.uint8))
    plt.axis('off')
    plt.tight_layout()
    plt.show()

    return label_map[predicted_class]

# === Example Usage ===
image_path = "/content/drive/MyDrive/TheOne/The IQ-OTHNCCD lung cancer dataset/Malignant cases/Malignant case (25).jpg"
prediction = predict_ct_scan_with_gradcam(image_path, model3)
print("Prediction:", prediction)

"""#Interpretation of XAI with LIME explanation"""

# === [Grad-CAM + LIME Enhanced Visualization for Explainable AI] ===
!pip install tf-keras-vis lime

import cv2
import numpy as np
import matplotlib.pyplot as plt
from tf_keras_vis.gradcam import Gradcam
from tf_keras_vis.utils.scores import CategoricalScore
import tensorflow as tf
from lime import lime_image
from skimage.segmentation import mark_boundaries

label_map = {0: 'Benign', 1: 'Malignant', 2: 'Normal'}

def predict_ct_scan_with_gradcam(image_path, model, img_size=256):
    """
    Predicts the class of a CT scan using the provided CNN model and visualizes Grad-CAM + LIME heatmap.
    """
    # Load and preprocess the image
    img = cv2.imread(image_path, 0)
    img_resized = cv2.resize(img, (img_size, img_size))
    img_normalized = img_resized.astype("float32") / 255.0
    img_input = img_normalized.reshape(1, img_size, img_size, 1)

    # Predict class
    pred = model.predict(img_input)
    predicted_class = np.argmax(pred)

    # Prepare input for Grad-CAM (1-channel)
    def model_modifier(m):
        m.layers[-1].activation = tf.keras.activations.linear

    score = CategoricalScore(predicted_class)
    gradcam = Gradcam(model, model_modifier=model_modifier, clone=True)
    cam = gradcam(score, img_input, penultimate_layer=-4)
    heatmap = cam[0]

    # Grad-CAM Visualization Adjustments
    heatmap_rescaled = cv2.resize(heatmap, (img_size, img_size))
    heatmap_clipped = np.clip(heatmap_rescaled, 0.2, 1)
    heatmap_normalized = (heatmap_clipped - heatmap_clipped.min()) / (heatmap_clipped.max() - heatmap_clipped.min())
    heatmap_color = cv2.applyColorMap(np.uint8(255 * heatmap_normalized), cv2.COLORMAP_JET)
    original_bgr = cv2.cvtColor(img_resized, cv2.COLOR_GRAY2BGR)
    overlay = cv2.addWeighted(original_bgr.astype(np.uint8), 0.6, heatmap_color, 0.4, 0)

    # === LIME Explanation ===
    def preprocess_for_lime(img):
        img = cv2.resize(img, (img_size, img_size))
        img = img.astype("float32") / 255.0
        return img.reshape(img_size, img_size, 1)

    def predict_fn(images):
        images = np.array(images)
        if images.shape[-1] == 3:
            images = np.mean(images, axis=-1, keepdims=True)
        return model.predict(images)

    lime_explainer = lime_image.LimeImageExplainer()
    explanation = lime_explainer.explain_instance(
        image=original_bgr.astype(np.uint8),
        classifier_fn=predict_fn,
        top_labels=1,
        hide_color=0,
        num_samples=1000
    )

    lime_img, mask = explanation.get_image_and_mask(
        label=predicted_class,
        positive_only=True,
        num_features=5,
        hide_rest=False
    )

    # === Display Combined Explanations ===
    plt.figure(figsize=(18, 6))
    plt.subplot(1, 4, 1)
    plt.title('Original Image')
    plt.imshow(img_resized, cmap='gray')
    plt.axis('off')

    plt.subplot(1, 4, 2)
    plt.title('Grad-CAM Heatmap')
    plt.imshow(heatmap_normalized, cmap='jet')
    plt.axis('off')

    plt.subplot(1, 4, 3)
    plt.title(f'Overlay - Predicted: {label_map[predicted_class]}')
    plt.imshow(overlay)
    plt.axis('off')

    plt.subplot(1, 4, 4)
    plt.title('LIME Explanation')
    plt.imshow(mark_boundaries(lime_img, mask))
    plt.axis('off')

    plt.tight_layout()
    plt.show()

    return label_map[predicted_class]

# === Example Usage ===
image_path = "/content/drive/MyDrive/TheOne/The IQ-OTHNCCD lung cancer dataset/Malignant cases/Malignant case (25).jpg"
prediction = predict_ct_scan_with_gradcam(image_path, model3)
print("Prediction:", prediction)

"""#Radiomic feature implementation with SVM and Random Forest"""

!pip install lime shap mlxtend scikit-learn matplotlib pandas

import numpy as np
import pandas as pd
from scipy.stats import ranksums
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
import shap
import lime
import lime.lime_tabular
import matplotlib.pyplot as plt

# -----------------------------
# STEP 1: Filter Benign & Malignant
# -----------------------------
X_bin = X_norm[(y == 0) | (y == 1)]
y_bin = y[(y == 0) | (y == 1)]

# -----------------------------
# STEP 2: Wilcoxon Filtering
# -----------------------------
p_values = [ranksums(X_bin[y_bin == 0, i], X_bin[y_bin == 1, i])[1] for i in range(X_bin.shape[1])]
top_indices = np.argsort(p_values)[:6]  # Top 6 features
X_filtered = X_bin[:, top_indices]

# Optional: feature names for interpretability
feature_names = [f'F{i}' for i in top_indices]

# -----------------------------
# STEP 3: Train/Test Split
# -----------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X_filtered, y_bin, test_size=0.2, random_state=42, stratify=y_bin)

# -----------------------------
# STEP 4: Train Classifiers
# -----------------------------
svm_clf = SVC(kernel='linear', probability=True)
svm_clf.fit(X_train, y_train)

rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)
rf_clf.fit(X_train, y_train)

# -----------------------------
# STEP 5: Evaluation
# -----------------------------
print("\n--- SVM Classification Report ---")
print(classification_report(y_test, svm_clf.predict(X_test)))

print("\n--- Random Forest Classification Report ---")
print(classification_report(y_test, rf_clf.predict(X_test)))

# -----------------------------
# STEP 6: SHAP for SVM
# -----------------------------
explainer = shap.Explainer(svm_clf.predict, X_train)
shap_values = explainer(X_test)

print("\nPlotting SHAP summary for SVM...")
shap.plots.beeswarm(shap_values, show=False)
plt.title("SHAP Summary Plot (SVM)")
plt.show()

# -----------------------------
# STEP 7: LIME for SVM
# -----------------------------
explainer_lime = lime.lime_tabular.LimeTabularExplainer(
    training_data=X_train,
    feature_names=feature_names,
    class_names=['Benign', 'Malignant'],
    mode='classification'
)

print("\nLIME Explanation for First Test Instance:")
exp = explainer_lime.explain_instance(X_test[0], svm_clf.predict_proba, num_features=6)
exp.show_in_notebook(show_all=False)  # Or use: exp.as_list()

# -----------------------------
# STEP 8: Export Data
# -----------------------------
df_export = pd.DataFrame(X_filtered, columns=feature_names)
df_export['label'] = y_bin
df_export.to_csv("radiomic_features_top6.csv", index=False)
print("Exported radiomic_features_top6.csv")

!pip install pyradiomics
!pip install SimpleITK
!pip install radiomics
import SimpleITK as sitk
from radiomics.featureextractor import RadiomicsFeatureExtractor # Corrected import
import cv2
import numpy as np
import pandas as pd

# Radiomics extractor (default settings, can be tuned)
extractor = RadiomicsFeatureExtractor() # Corrected instantiation

def extract_intra_peri_features(image_path, nodule_mask=None, img_size=256, margin=10):
    """
    Extract intranodular and perinodular radiomic features from a CT image.

    Args:
        image_path (str): Path to CT scan image
        nodule_mask (ndarray or None): Binary mask for the nodule region. If None, approximate with thresholding.
        img_size (int): Resize dimension
        margin (int): Margin size (in pixels) for perinodular region
    Returns:
        dict: Combined radiomic features
    """
    # Load and preprocess image
    img = cv2.imread(image_path, 0)
    img_resized = cv2.resize(img, (img_size, img_size))
    itk_img = sitk.GetImageFromArray(img_resized.astype(np.float32))

    # Create / Approximate mask
    if nodule_mask is None:
        # crude segmentation by threshold + largest contour
        _, thresh = cv2.threshold(img_resized, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        mask = np.zeros_like(img_resized, dtype=np.uint8)
        if contours:
            cv2.drawContours(mask, [max(contours, key=cv2.contourArea)], -1, 255, -1)
    else:
        mask = cv2.resize(nodule_mask, (img_size, img_size))

    itk_mask = sitk.GetImageFromArray(mask.astype(np.uint8))

    # Intranodular features
    features_intra = extractor.execute(itk_img, itk_mask)
    features_intra = {f"Intra_{k}": v for k, v in features_intra.items() if isinstance(v, (int, float))}

    # Perinodular mask (dilate the mask and subtract original)
    kernel = np.ones((margin, margin), np.uint8)
    dilated = cv2.dilate(mask, kernel, iterations=1)
    perimask = dilated - mask
    itk_peri = sitk.GetImageFromArray(perimask.astype(np.uint8))

    # Perinodular features
    features_peri = extractor.execute(itk_img, itk_peri)
    features_peri = {f"Peri_{k}": v for k, v in features_peri.items() if isinstance(v, (int, float))}


    # Merge both feature sets
    features = {**features_intra, **features_peri}
    return features

# Example usage
image_path = "/content/drive/MyDrive/TheOne/The IQ-OTHNCCD lung cancer dataset/Malignant cases/Malignant case (1).jpg"
features = extract_intra_peri_features(image_path)
df = pd.DataFrame([features])
print(df.head())

# Check current python
!python --version

# Switch to Python 3.10
!sudo apt-get install python3.10 python3.10-dev
!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.10 2
!sudo update-alternatives --config python3

# After switching, reinstall pip and needed packages
!curl -sS https://bootstrap.pypa.io/get-pip.py | python3
!pip install pyradiomics SimpleITK

import SimpleITK as sitk
from radiomics import featureextractor

extractor = featureextractor.RadiomicsFeatureExtractor()
print("PyRadiomics extractor initialized!")

!pip install git+https://github.com/AIM-Harvard/pyradiomics.git

!pip uninstall -y radiomics
!pip install pyradiomics
import SimpleITK as sitk
from radiomics import featureextractor

extractor = featureextractor.RadiomicsFeatureExtractor()
print("PyRadiomics extractor initialized!")


# Radiomics extractor (default settings, can be tuned)
extractor = featureextractor.RadiomicsFeatureExtractor() # Corrected instantiation

def extract_intra_peri_features(image_path, nodule_mask=None, img_size=256, margin=10):
    """
    Extract intranodular and perinodular radiomic features from a CT image.

    Args:
        image_path (str): Path to CT scan image
        nodule_mask (ndarray or None): Binary mask for the nodule region. If None, approximate with thresholding.
        img_size (int): Resize dimension
        margin (int): Margin size (in pixels) for perinodular region
    Returns:
        dict: Combined radiomic features
    """
    # Load and preprocess image
    img = cv2.imread(image_path, 0)
    img_resized = cv2.resize(img, (img_size, img_size))
    itk_img = sitk.GetImageFromArray(img_resized.astype(np.float32))

    # Create / Approximate mask
    if nodule_mask is None:
        # crude segmentation by threshold + largest contour
        _, thresh = cv2.threshold(img_resized, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        mask = np.zeros_like(img_resized, dtype=np.uint8)
        if contours:
            cv2.drawContours(mask, [max(contours, key=cv2.contourArea)], -1, 255, -1)
    else:
        mask = cv2.resize(nodule_mask, (img_size, img_size))

    itk_mask = sitk.GetImageFromArray(mask.astype(np.uint8))

    # Intranodular features
    features_intra = extractor.execute(itk_img, itk_mask)
    features_intra = {f"Intra_{k}": v for k, v in features_intra.items() if isinstance(v, (int, float))}

    # Perinodular mask (dilate the mask and subtract original)
    kernel = np.ones((margin, margin), np.uint8)
    dilated = cv2.dilate(mask, kernel, iterations=1)
    perimask = dilated - mask
    itk_peri = sitk.GetImageFromArray(perimask.astype(np.uint8))

    # Perinodular features
    features_peri = extractor.execute(itk_img, itk_peri)
    features_peri = {f"Peri_{k}": v for k, v in features_peri.items() if isinstance(v, (int, float))}


    # Merge both feature sets
    features = {**features_intra, **features_peri}
    return features

# Example usage
image_path = "/content/drive/MyDrive/TheOne/The IQ-OTHNCCD lung cancer dataset/Malignant cases/Malignant case (1).jpg"
features = extract_intra_peri_features(image_path)
df = pd.DataFrame([features])
print(df.head())